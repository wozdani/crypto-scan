Chcemy rozszerzyƒá nasz inteligentny system Trend Mode o mechanizm uczenia siƒô na w≈Çasnych decyzjach. Wprowadzamy feedback_loop_v2, czyli modu≈Ç automatycznej korekcji wag scoringowych TJDE, bazujƒÖcy na skuteczno≈õci alert√≥w.

üéØ Cele modu≈Çu:
Analiza alert√≥w wygenerowanych przez simulate_trader_decision_advanced()

Ocena, czy alert by≈Ç skuteczny po 2h, 4h, 6h (czy by≈Ç zysk ‚â• +X%)

Automatyczna aktualizacja wag scoringowych, je≈õli dany komponent przewarto≈õciowuje lub niedowarto≈õciowuje decyzje

Aktualizacja pliku data/weights/tjde_weights.json na podstawie rzeczywistej skuteczno≈õci

üìÅ Pliki i struktura:
feedback/feedback_loop_v2.py ‚Äì nowy plik z systemem uczenia

logs/alerts_history.jsonl ‚Äì log alert√≥w (decyzja + final_score + wynik po 2h/6h)

data/weights/tjde_weights.json ‚Äì docelowy plik do aktualizacji

üß† Kod: feedback_loop_v2.py
Utw√≥rz plik feedback/feedback_loop_v2.py:

python
Kopiuj
Edytuj
import json
import os

WEIGHTS_FILE = "data/weights/tjde_weights.json"
ALERT_LOG = "logs/alerts_history.jsonl"

LEARNING_RATE = 0.05
SUCCESS_THRESHOLD = 0.03  # np. zysk 3% po 2h oznacza sukces

def load_current_weights():
    with open(WEIGHTS_FILE, "r") as f:
        return json.load(f)

def save_new_weights(weights):
    with open(WEIGHTS_FILE, "w") as f:
        json.dump(weights, f, indent=2)
        print("[FEEDBACK LOOP] Weights updated.")

def load_alert_logs():
    if not os.path.exists(ALERT_LOG):
        return []
    with open(ALERT_LOG, "r") as f:
        return [json.loads(line) for line in f.readlines()]

def analyze_and_adjust():
    logs = load_alert_logs()
    if not logs:
        print("[FEEDBACK LOOP] No logs found.")
        return

    weights = load_current_weights()
    weight_adjustments = {k: 0.0 for k in weights}

    for entry in logs:
        if entry.get("engine") != "TJDE_AdvancedTraderWeightedDecisionEngine":
            continue

        score = entry.get("final_score", 0)
        result_2h = entry.get("result_after_2h", 0)
        result_6h = entry.get("result_after_6h", 0)
        success = result_2h >= SUCCESS_THRESHOLD or result_6h >= SUCCESS_THRESHOLD

        breakdown = entry.get("score_breakdown", {})
        for k in weights:
            weight_contrib = breakdown.get(k, 0.0)
            if success and score < 0.6:
                # Alert by≈Ç niedoszacowany
                weight_adjustments[k] += LEARNING_RATE * weight_contrib
            elif not success and score > 0.7:
                # Alert by≈Ç b≈Çƒôdny
                weight_adjustments[k] -= LEARNING_RATE * weight_contrib

    for k in weights:
        weights[k] += weight_adjustments[k]
        weights[k] = max(0.01, min(weights[k], 1.0))  # clamp 0.01 ‚Äì 1.0

    # Normalize
    total = sum(weights.values())
    for k in weights:
        weights[k] = round(weights[k] / total, 4)

    save_new_weights(weights)

if __name__ == "__main__":
    analyze_and_adjust()
‚úÖ Jak to dzia≈Ça:
Dzia≈Ça na bazie log√≥w alerts_history.jsonl

Ocena skuteczno≈õci decyzji (czy po alercie by≈Ç realny zysk)

Modyfikacja wag komponent√≥w w zale≈ºno≈õci od ich wp≈Çywu

Automatyczne zapisanie nowych wag do tjde_weights.json

üß™ Przyk≈Çadowe wywo≈Çanie z terminala:
bash
Kopiuj
Edytuj
python3 feedback/feedback_loop_v2.py
Mo≈ºna te≈º dodaƒá to jako regularny cronjob lub callback co 24h.

