#!/usr/bin/env python3
"""
Pump Analysis System - Automatic detection and analysis of crypto pumps
Analyzes historical pump data and generates GPT insights for learning purposes
"""

import os
import time
import json
import re
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from openai import OpenAI
import logging
from learning_system import LearningSystem

# Configure logging with DEBUG level for detailed pump analysis debugging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class PumpEvent:
    """Data class for storing pump event information"""
    symbol: str
    start_time: datetime
    price_before: float
    price_peak: float
    price_increase_pct: float
    duration_minutes: int
    volume_spike: float

class BybitDataFetcher:
    """Handles data fetching from Bybit API"""
    
    def __init__(self):
        self.base_url = "https://api.bybit.com"
        self.api_key = os.getenv('BYBIT_API_KEY', '')
        self.api_secret = os.getenv('BYBIT_SECRET_KEY', '')
        
        # Headers for all requests
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json',
            'Content-Type': 'application/json'
        }
        
        if self.api_key:
            self.headers['X-BAPI-API-KEY'] = self.api_key
    
    def _get_authenticated_headers(self, params=None):
        """Generate authenticated headers for Bybit API using same logic as crypto-scan"""
        if not self.api_key or not self.api_secret:
            return {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/json'
            }
        
        import hmac
        import hashlib
        
        timestamp = str(int(time.time() * 1000))
        recv_window = "5000"
        
        if params:
            param_str = "&".join([f"{k}={v}" for k, v in sorted(params.items())])
            raw_str = timestamp + self.api_key + recv_window + param_str
        else:
            raw_str = timestamp + self.api_key + recv_window
        
        signature = hmac.new(
            self.api_secret.encode('utf-8'),
            raw_str.encode('utf-8'),
            hashlib.sha256
        ).hexdigest()
        
        return {
            "X-BAPI-API-KEY": self.api_key,
            "X-BAPI-SIGN": signature,
            "X-BAPI-SIGN-TYPE": "2",
            "X-BAPI-TIMESTAMP": timestamp,
            "X-BAPI-RECV-WINDOW": recv_window,
            "Content-Type": "application/json"
        }
        
    def get_kline_data(self, symbol: str, interval: str = "5", start_time: int = None, limit: int = 200) -> List[Dict]:
        """
        Fetch kline data from Bybit
        
        Args:
            symbol: Trading symbol (e.g., 'BTCUSDT')
            interval: Kline interval (1, 3, 5, 15, 30, 60, 120, 240, 360, 720, D, W, M)
            start_time: Start timestamp in milliseconds
            limit: Number of data points (max 1000)
        """
        endpoint = f"{self.base_url}/v5/market/kline"
        
        params = {
            'category': 'linear',
            'symbol': symbol,
            'interval': interval,
            'limit': limit
        }
        
        if start_time:
            params['start'] = start_time
            
        try:
            logger.debug(f"üì° Bybit API request: {endpoint} with params: {params}")
            headers = self._get_authenticated_headers(params)
            response = requests.get(endpoint, params=params, headers=headers)
            response.raise_for_status()
            data = response.json()
            
            if data['retCode'] == 0:
                result_data = data['result']['list']
                logger.debug(f"‚úÖ Bybit API success for {symbol}: {len(result_data)} candles retrieved")
                return result_data
            else:
                logger.error(f"Bybit API error for {symbol}: {data['retMsg']}")
                return []
                
        except Exception as e:
            logger.error(f"Error fetching data for {symbol}: {e}")
            return []
    
    def get_active_symbols(self, limit: int = None) -> List[str]:
        """
        Get active trading symbols from Bybit futures perpetual (linear category)
        Uses same logic as crypto-scan main scanner
        """
        logger.info(f"üìä Fetching USDT perpetual futures symbols from Bybit...")
        
        symbols = set()
        cursor = ""
        
        try:
            while True:
                # Use linear category for futures perpetual (same as crypto-scan)
                endpoint = f"{self.base_url}/v5/market/tickers"
                params = {
                    "category": "linear",
                    "limit": 1000
                }
                if cursor:
                    params["cursor"] = cursor
                
                headers = self._get_authenticated_headers(params)
                
                logger.debug(f"üîó Fetching symbols with cursor: {cursor}")
                response = requests.get(endpoint, params=params, headers=headers, timeout=20)
                response.raise_for_status()
                data = response.json()
                
                logger.debug(f"üìä API Response: retCode={data.get('retCode')}, symbols_count={len(data.get('result', {}).get('list', []))}")
                
                if data.get('retCode') == 0:
                    page_symbols = data.get('result', {}).get('list', [])
                    
                    usdt_count = 0
                    for item in page_symbols:
                        symbol = item.get('symbol', '')
                        status = item.get('status', '')
                        if symbol and symbol.endswith('USDT') and status == 'Trading':
                            symbols.add(symbol)
                            usdt_count += 1
                    
                    cursor = data.get('result', {}).get('nextPageCursor')
                    logger.info(f"üìÑ Page processed: {usdt_count} USDT symbols added (total: {len(symbols)}), next_cursor: {bool(cursor)}")
                    
                    if not cursor:
                        logger.info(f"‚úÖ Symbol fetching complete - no more pages")
                        break
                        
                    time.sleep(0.1)  # Rate limiting
                else:
                    logger.error(f"Bybit API error: {data.get('retMsg', 'Unknown error')}")
                    logger.debug(f"Full API response: {data}")
                    break
                    
        except Exception as e:
            logger.error(f"Error fetching symbols from Bybit: {e}")
            logger.debug(f"Exception details: {type(e).__name__}: {str(e)}")
        
        if symbols:
            symbol_list = sorted(list(symbols))
            logger.info(f"‚úÖ Retrieved {len(symbol_list)} USDT perpetual futures symbols")
            logger.info(f"üîù First 10: {symbol_list[:10]}")
            return symbol_list
        
        # Fallback only if API completely fails
        fallback_symbols = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT',
            'XRPUSDT', 'DOTUSDT', 'LINKUSDT', 'LTCUSDT', 'BCHUSDT',
            'XLMUSDT', 'UNIUSDT', 'FILUSDT', 'TRXUSDT', 'ETCUSDT',
            'NEARUSDT', 'ATOMUSDT', 'ALGOUSDT', 'VETUSDT', 'ICPUSDT',
            'THETAUSDT', 'HBARUSDT', 'EGLDUSDT', 'AAVEUSDT', 'EOSUSDT',
            'AXSUSDT', 'SANDUSDT', 'MANAUSDT', 'GALAUSDT', 'PAWSUSDT'
        ]
        logger.warning("‚ö†Ô∏è Using fallback symbol list due to API failure")
        return fallback_symbols

class PumpDetector:
    """Detects pump events in price data"""
    
    def __init__(self, min_increase_pct: float = 15.0, detection_window_minutes: int = 30):
        self.min_increase_pct = min_increase_pct
        self.detection_window_minutes = detection_window_minutes
        
    def detect_pumps_in_data(self, kline_data: List[List], symbol: str) -> List[PumpEvent]:
        """
        Detect pump events in kline data
        
        Args:
            kline_data: List of kline data from Bybit API
            symbol: Trading symbol
            
        Returns:
            List of detected pump events
        """
        if len(kline_data) < 12:  # Need at least 1 hour of 5-min data
            return []
            
        pumps = []
        
        # Convert to DataFrame for easier analysis
        df = pd.DataFrame(kline_data, columns=[
            'start_time', 'open', 'high', 'low', 'close', 'volume', 'turnover'
        ])
        
        # Convert to numeric
        df['open'] = pd.to_numeric(df['open'])
        df['high'] = pd.to_numeric(df['high'])
        df['low'] = pd.to_numeric(df['low'])
        df['close'] = pd.to_numeric(df['close'])
        df['volume'] = pd.to_numeric(df['volume'])
        df['start_time'] = pd.to_numeric(df['start_time'])
        
        # Convert timestamp to datetime
        df['datetime'] = pd.to_datetime(df['start_time'], unit='ms')
        df = df.sort_values('datetime')
        
        # Rolling window analysis for pump detection
        window_size = self.detection_window_minutes // 5  # 5-min candles
        
        for i in range(window_size, len(df)):
            window_start = i - window_size
            window_data = df.iloc[window_start:i+1]
            
            price_start = window_data.iloc[0]['close']
            price_peak = window_data['high'].max()
            
            increase_pct = ((price_peak - price_start) / price_start) * 100
            
            if increase_pct >= self.min_increase_pct:
                # Calculate additional metrics
                volume_before = df.iloc[max(0, window_start-6):window_start]['volume'].mean()
                volume_during = window_data['volume'].mean()
                volume_spike = volume_during / volume_before if volume_before > 0 else 1
                
                pump_event = PumpEvent(
                    symbol=symbol,
                    start_time=window_data.iloc[0]['datetime'],
                    price_before=price_start,
                    price_peak=price_peak,
                    price_increase_pct=increase_pct,
                    duration_minutes=self.detection_window_minutes,
                    volume_spike=volume_spike
                )
                
                pumps.append(pump_event)
                
                # Skip overlapping windows
                i += window_size
                
        return pumps

class PrePumpAnalyzer:
    """Analyzes pre-pump conditions"""
    
    def __init__(self, bybit_fetcher: BybitDataFetcher):
        self.bybit = bybit_fetcher
        
    def analyze_pre_pump_conditions(self, pump_event: PumpEvent) -> Dict:
        """
        Analyze conditions 60 minutes before pump
        
        Args:
            pump_event: Detected pump event
            
        Returns:
            Dictionary with pre-pump analysis
        """
        # Calculate time 60 minutes before pump
        pre_pump_start = pump_event.start_time - timedelta(minutes=60)
        start_timestamp = int(pre_pump_start.timestamp() * 1000)
        
        # Get 1-hour of data before pump (12 x 5-min candles)
        logger.info(f"üìä Fetching pre-pump data for {pump_event.symbol}: 60min before {pump_event.start_time}")
        pre_pump_data = self.bybit.get_kline_data(
            symbol=pump_event.symbol,
            interval="5",
            start_time=start_timestamp,
            limit=12
        )
        
        if not pre_pump_data:
            logger.warning(f"‚ö†Ô∏è No pre-pump data available for {pump_event.symbol} at timestamp {start_timestamp}")
            return None
        
        logger.info(f"‚úÖ Retrieved {len(pre_pump_data)} candles for {pump_event.symbol} pre-pump analysis")
            
        # Convert to DataFrame
        df = pd.DataFrame(pre_pump_data, columns=[
            'start_time', 'open', 'high', 'low', 'close', 'volume', 'turnover'
        ])
        
        # Convert to numeric
        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = pd.to_numeric(df[col])
            
        # Calculate indicators
        analysis = {
            'symbol': pump_event.symbol,
            'pump_start_time': pump_event.start_time.isoformat(),
            'pump_increase_pct': pump_event.price_increase_pct,
            'pre_pump_period': '60_minutes_before',
            'kline_data': df.to_dict('records'),  # Add raw kline data for testing
            
            # Price analysis
            'price_volatility': df['close'].std(),
            'price_trend': self._calculate_trend(df['close']),
            'price_compression': self._detect_compression(df),
            
            # Volume analysis
            'avg_volume': df['volume'].mean(),
            'volume_trend': self._calculate_trend(df['volume']),
            'volume_spikes': self._detect_volume_spikes(df),
            
            # Technical indicators
            'rsi': self._calculate_rsi(df['close']),
            'vwap': self._calculate_vwap(df),
            'fake_rejects': self._detect_fake_rejects(df),
            
            # Market structure
            'support_resistance': self._identify_support_resistance(df),
            'liquidity_gaps': self._detect_liquidity_gaps(df),
        }
        
        return analysis
    
    def _calculate_trend(self, prices: pd.Series) -> str:
        """Calculate price trend direction"""
        if len(prices) < 2:
            return "neutral"
            
        first_half = prices[:len(prices)//2].mean()
        second_half = prices[len(prices)//2:].mean()
        
        # Prevent division by zero
        if first_half == 0 or pd.isna(first_half) or pd.isna(second_half):
            return "neutral"
        
        change_pct = ((second_half - first_half) / first_half) * 100
        
        if change_pct > 1:
            return "bullish"
        elif change_pct < -1:
            return "bearish"
        else:
            return "neutral"
    
    def _detect_compression(self, df: pd.DataFrame) -> Dict:
        """Detect price compression patterns"""
        price_range = df['high'].max() - df['low'].min()
        avg_price = df['close'].mean()
        compression_ratio = (price_range / avg_price) * 100
        
        return {
            'compression_ratio_pct': compression_ratio,
            'is_compressed': compression_ratio < 2.0  # Less than 2% range
        }
    
    def _detect_volume_spikes(self, df: pd.DataFrame) -> List[Dict]:
        """Detect volume spikes in pre-pump period"""
        avg_volume = df['volume'].mean()
        spikes = []
        
        for i, row in df.iterrows():
            if row['volume'] > avg_volume * 2:  # 2x average volume
                spikes.append({
                    'time_minutes_before_pump': (len(df) - i) * 5,
                    'volume_multiplier': row['volume'] / avg_volume
                })
                
        return spikes
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> float:
        """Calculate RSI indicator"""
        if len(prices) < period:
            return 50.0  # Neutral RSI
            
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        
        return float(rsi.iloc[-1]) if not pd.isna(rsi.iloc[-1]) else 50.0
    
    def _calculate_vwap(self, df: pd.DataFrame) -> Dict:
        """Calculate VWAP and analyze position relative to it"""
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        vwap = (typical_price * df['volume']).sum() / df['volume'].sum()
        
        final_price = df['close'].iloc[-1]
        
        return {
            'vwap_value': float(vwap),
            'price_vs_vwap_pct': ((final_price - vwap) / vwap) * 100,
            'above_vwap': final_price > vwap
        }
    
    def _detect_fake_rejects(self, df: pd.DataFrame) -> List[Dict]:
        """Detect fake rejection patterns (long wicks followed by recovery)"""
        fake_rejects = []
        
        for i in range(1, len(df)):
            current = df.iloc[i]
            previous = df.iloc[i-1]
            
            # Calculate wick sizes
            lower_wick = abs(current['close'] - current['low']) / current['close']
            upper_wick = abs(current['high'] - current['close']) / current['close']
            
            # Detect long lower wick followed by recovery
            if (lower_wick > 0.02 and  # Wick > 2%
                current['close'] > previous['close'] and  # Recovery
                current['close'] > (current['low'] + (current['high'] - current['low']) * 0.6)):  # Close in upper 40%
                
                fake_rejects.append({
                    'time_minutes_before_pump': (len(df) - i) * 5,
                    'wick_size_pct': lower_wick * 100,
                    'recovery_strength': ((current['close'] - current['low']) / (current['high'] - current['low'])) * 100
                })
                
        return fake_rejects
    
    def _identify_support_resistance(self, df: pd.DataFrame) -> Dict:
        """Identify key support and resistance levels"""
        highs = df['high'].values
        lows = df['low'].values
        
        # Simple pivot identification
        resistance_levels = []
        support_levels = []
        
        for i in range(1, len(highs)-1):
            if highs[i] > highs[i-1] and highs[i] > highs[i+1]:
                resistance_levels.append(highs[i])
            if lows[i] < lows[i-1] and lows[i] < lows[i+1]:
                support_levels.append(lows[i])
        
        return {
            'resistance_levels': resistance_levels,
            'support_levels': support_levels,
            'key_resistance': max(resistance_levels) if resistance_levels else None,
            'key_support': min(support_levels) if support_levels else None
        }
    
    def _detect_liquidity_gaps(self, df: pd.DataFrame) -> List[Dict]:
        """Detect potential liquidity gaps"""
        gaps = []
        
        for i in range(1, len(df)):
            current = df.iloc[i]
            previous = df.iloc[i-1]
            
            # Gap up
            if current['low'] > previous['high']:
                gap_size = ((current['low'] - previous['high']) / previous['high']) * 100
                gaps.append({
                    'type': 'gap_up',
                    'size_pct': gap_size,
                    'time_minutes_before_pump': (len(df) - i) * 5
                })
            
            # Gap down
            elif current['high'] < previous['low']:
                gap_size = ((previous['low'] - current['high']) / current['high']) * 100
                gaps.append({
                    'type': 'gap_down',
                    'size_pct': gap_size,
                    'time_minutes_before_pump': (len(df) - i) * 5
                })
        
        return gaps

class GPTAnalyzer:
    """Handles GPT analysis of pre-pump conditions"""
    
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
        
    def generate_pump_analysis(self, pre_pump_data: Dict) -> str:
        """
        Generate GPT analysis of pre-pump conditions
        
        Args:
            pre_pump_data: Dictionary with pre-pump analysis data
            
        Returns:
            GPT analysis text
        """
        
        # Format data for GPT prompt
        prompt = self._format_analysis_prompt(pre_pump_data)
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",  # the newest OpenAI model is "gpt-4o" which was released May 13, 2024. do not change this unless explicitly requested by the user
                messages=[
                    {
                        "role": "system",
                        "content": """Jeste≈õ ekspertem analizy technicznej rynku kryptowalut. 
                        Analizujesz warunki pre-pump (60 minut przed nag≈Çym wzrostem ceny) aby 
                        zidentyfikowaƒá wzorce i sygna≈Çy, kt√≥re mog≈Çy przewidzieƒá nadchodzƒÖcy pump.
                        
                        Odpowiadaj w jƒôzyku polskim. Skup siƒô na:
                        1. Identyfikacji kluczowych sygna≈Ç√≥w pre-pump
                        2. Analizie struktury rynku przed ruchem
                        3. Ocenie si≈Çy sygna≈Ç√≥w akumulacyjnych
                        4. Wskazaniu najwa≈ºniejszych wska≈∫nik√≥w ostrzegawczych
                        5. Podsumowaniu lekcji do zastosowania w przysz≈Ço≈õci"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"GPT analysis error: {e}")
            return f"B≈ÇƒÖd podczas generowania analizy GPT: {e}"

    def generate_detector_function(self, pre_pump_data: Dict, pump_event: 'PumpEvent') -> str:
        """
        Generate Python detector function based on pre-pump analysis
        
        Args:
            pre_pump_data: Dictionary with pre-pump analysis data
            pump_event: PumpEvent with pump details
            
        Returns:
            Python function code as string
        """
        
        # Create prompt for function generation
        prompt = self._format_detector_prompt(pre_pump_data, pump_event)
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",  # the newest OpenAI model is "gpt-4o" which was released May 13, 2024. do not change this unless explicitly requested by the user
                messages=[
                    {
                        "role": "system",
                        "content": """Jeste≈õ ekspertem programowania w Pythonie i analizy technicznej.
                        Tworzysz funkcje detektor√≥w pre-pump na podstawie rzeczywistych przypadk√≥w pump'√≥w.
                        
                        WYMAGANIA:
                        1. U≈ºywaj pandas DataFrame z kolumnami: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'vwap', 'rsi']
                        2. Zwracaj True je≈õli setup zosta≈Ç spe≈Çniony, False w przeciwnym razie
                        3. Nie u≈ºywaj zewnƒôtrznych bibliotek poza pandas i numpy
                        4. Nazwa funkcji: detect_{symbol}_{date}_preconditions()
                        5. Dodaj docstring z opisem wzorca
                        6. Implementuj konkretnƒÖ logikƒô na podstawie dostarczonych danych
                        7. U≈ºywaj realistic thresholds bazujƒÖcych na danych wej≈õciowych"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=2000,
                temperature=0.3  # Lower temperature for more consistent code generation
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"GPT detector function generation error: {e}")
            return f"# Error generating detector function: {e}"
    
    def _format_analysis_prompt(self, data: Dict) -> str:
        """Format pre-pump data into GPT prompt"""
        
        prompt = f"""
ANALIZA PRE-PUMP - {data['symbol']}

=== INFORMACJE O PUMPIE ===
‚Ä¢ Symbol: {data['symbol']}
‚Ä¢ Czas rozpoczƒôcia pumpu: {data['pump_start_time']}
‚Ä¢ Wzrost ceny: +{data['pump_increase_pct']:.1f}%
‚Ä¢ Okres analizy: {data['pre_pump_period']}

=== ANALIZA CENY (60 min przed pumpem) ===
‚Ä¢ Zmienno≈õƒá ceny: {data['price_volatility']:.6f}
‚Ä¢ Trend cenowy: {data['price_trend']}
‚Ä¢ Kompresja cenowa: {data['price_compression']['compression_ratio_pct']:.2f}% (skompresowana: {data['price_compression']['is_compressed']})

=== ANALIZA WOLUMENU ===
‚Ä¢ ≈öredni wolumen: {data['avg_volume']:.0f}
‚Ä¢ Trend wolumenu: {data['volume_trend']}
‚Ä¢ Wykryte spike'i wolumenu: {len(data['volume_spikes'])}
"""

        if data['volume_spikes']:
            prompt += "\n‚Ä¢ Szczeg√≥≈Çy spike'√≥w wolumenu:\n"
            for spike in data['volume_spikes']:
                prompt += f"  - {spike['time_minutes_before_pump']} min przed: {spike['volume_multiplier']:.1f}x ≈õredniego\n"

        prompt += f"""
=== WSKA≈πNIKI TECHNICZNE ===
‚Ä¢ RSI: {data['rsi']:.1f}
‚Ä¢ VWAP: {data['vwap']['vwap_value']:.6f}
‚Ä¢ Pozycja vs VWAP: {data['vwap']['price_vs_vwap_pct']:.2f}% ({'powy≈ºej' if data['vwap']['above_vwap'] else 'poni≈ºej'} VWAP)

=== STRUKTURY RYNKOWE ===
‚Ä¢ Fake reject'y wykryte: {len(data['fake_rejects'])}
"""

        if data['fake_rejects']:
            prompt += "‚Ä¢ Szczeg√≥≈Çy fake reject'√≥w:\n"
            for fr in data['fake_rejects']:
                prompt += f"  - {fr['time_minutes_before_pump']} min przed: wick {fr['wick_size_pct']:.1f}%, recovery {fr['recovery_strength']:.1f}%\n"

        if data['support_resistance']['key_support'] or data['support_resistance']['key_resistance']:
            support_text = f"{data['support_resistance']['key_support']:.6f}" if data['support_resistance']['key_support'] is not None else 'brak'
            resistance_text = f"{data['support_resistance']['key_resistance']:.6f}" if data['support_resistance']['key_resistance'] is not None else 'brak'
            prompt += f"""
‚Ä¢ Kluczowe poziomy:
  - Wsparcie: {support_text}
  - Op√≥r: {resistance_text}
"""

        if data['liquidity_gaps']:
            prompt += f"\n‚Ä¢ Luki p≈Çynno≈õciowe: {len(data['liquidity_gaps'])}\n"
            for gap in data['liquidity_gaps']:
                prompt += f"  - {gap['type']}: {gap['size_pct']:.2f}% ({gap['time_minutes_before_pump']} min przed)\n"

        prompt += """

=== ZADANIE ANALIZY ===
Na podstawie powy≈ºszych danych z 60 minut przed pumpem, przeanalizuj:

1. Jakie by≈Çy najwa≈ºniejsze sygna≈Çy ostrzegawcze?
2. Czy struktura rynku wskazywa≈Ça na przygotowania do ruchu?
3. Kt√≥re wska≈∫niki by≈Çy najbardziej predykcyjne?
4. Jakie wzorce akumulacji mo≈ºna zidentyfikowaƒá?
5. Co mo≈ºna by≈Ço zauwa≈ºyƒá wcze≈õniej, ≈ºeby przewidzieƒá pump?

Podaj konkretnƒÖ, praktycznƒÖ analizƒô z naciskiem na aplikowalno≈õƒá w przysz≈Çych sytuacjach.
"""

        return prompt

    def _format_detector_prompt(self, data: Dict, pump_event: 'PumpEvent') -> str:
        """Format data into prompt for detector function generation"""
        
        # Extract date from pump event
        pump_date = pump_event.start_time.strftime("%Y%m%d")
        symbol = pump_event.symbol
        
        prompt = f"""
GENEROWANIE FUNKCJI DETEKTORA PRE-PUMP

=== DANE PRZYPADKU ===
‚Ä¢ Symbol: {symbol}
‚Ä¢ Data pumpu: {pump_date}
‚Ä¢ Wzrost ceny: +{pump_event.price_increase_pct:.1f}%
‚Ä¢ Czas trwania: {pump_event.duration_minutes} minut

=== KLUCZOWE CHARAKTERYSTYKI PRE-PUMP ===
‚Ä¢ Kompresja cenowa: {data['price_compression']['compression_ratio_pct']:.2f}% (skompresowana: {data['price_compression']['is_compressed']})
‚Ä¢ Trend cenowy: {data['price_trend']}
‚Ä¢ RSI: {data['rsi']:.1f}
‚Ä¢ Pozycja vs VWAP: {data['vwap']['price_vs_vwap_pct']:.2f}% ({'powy≈ºej' if data['vwap']['above_vwap'] else 'poni≈ºej'})
‚Ä¢ ≈öredni wolumen: {data['avg_volume']:.0f}
‚Ä¢ Trend wolumenu: {data['volume_trend']}
‚Ä¢ Liczba volume spike'√≥w: {len(data['volume_spikes'])}
‚Ä¢ Liczba fake reject'√≥w: {len(data['fake_rejects'])}
‚Ä¢ Kluczowe wsparcie: {data['support_resistance']['key_support'] or 'brak'}
‚Ä¢ Kluczowy op√≥r: {data['support_resistance']['key_resistance'] or 'brak'}
"""

        if data['volume_spikes']:
            prompt += "\n=== SZCZEG√ì≈ÅY VOLUME SPIKE'√ìW ===\n"
            for spike in data['volume_spikes'][:3]:  # Top 3 spikes
                prompt += f"‚Ä¢ {spike['time_minutes_before_pump']} min przed: {spike['volume_multiplier']:.1f}x ≈õredniego\n"

        if data['fake_rejects']:
            prompt += "\n=== SZCZEG√ì≈ÅY FAKE REJECT'√ìW ===\n"
            for fr in data['fake_rejects'][:2]:  # Top 2 fake rejects
                prompt += f"‚Ä¢ {fr['time_minutes_before_pump']} min przed: wick {fr['wick_size_pct']:.1f}%, recovery {fr['recovery_strength']:.1f}%\n"

        prompt += f"""

=== ZADANIE ===
Wygeneruj funkcjƒô w jƒôzyku Python, kt√≥ra potrafi≈Çaby wykryƒá taki przypadek pre-pump.

WYMAGANIA FUNKCJI:
1. Nazwa: detect_{symbol}_{pump_date}_preconditions(df)
2. Parameter: pandas DataFrame z kolumnami ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'vwap', 'rsi']
3. Zwraca: True je≈õli setup zosta≈Ç spe≈Çniony, False w przeciwnym razie
4. U≈ºywaj tylko pandas i numpy (bez zewnƒôtrznych bibliotek)
5. Implementuj konkretnƒÖ logikƒô bazujƒÖcƒÖ na dostarczonych parametrach
6. Dodaj docstring opisujƒÖcy wzorzec

PODEJ≈öCIE DO IMPLEMENTACJI:
‚Ä¢ Sprawd≈∫ kompresjƒô cenowƒÖ (zakres high-low vs ≈õrednia)
‚Ä¢ Zweryfikuj pozycjƒô vs VWAP
‚Ä¢ Wykryj volume spike'y (por√≥wnaj z moving average)
‚Ä¢ Sprawd≈∫ fake reject'y (d≈Çugie wicki + recovery)
‚Ä¢ Uwzglƒôdnij trend RSI i ceny
‚Ä¢ U≈ºyj realistic thresholds bazujƒÖcych na podanych warto≈õciach

Wygeneruj kompletnƒÖ, dzia≈ÇajƒÖcƒÖ funkcjƒô Python gotowƒÖ do zapisania w pliku.
"""

        return prompt

class TelegramNotifier:
    """Handles Telegram notifications"""
    
    def __init__(self, bot_token: str, chat_id: str):
        self.bot_token = bot_token
        self.chat_id = chat_id
        self.base_url = f"https://api.telegram.org/bot{bot_token}"
        
    def send_message(self, message: str) -> bool:
        """
        Send message to Telegram chat
        
        Args:
            message: Message text to send
            
        Returns:
            True if sent successfully, False otherwise
        """
        
        url = f"{self.base_url}/sendMessage"
        
        # Split long messages
        max_length = 4000
        if len(message) > max_length:
            parts = [message[i:i+max_length] for i in range(0, len(message), max_length)]
            for i, part in enumerate(parts):
                success = self._send_single_message(f"üìä CZƒò≈öƒÜ {i+1}/{len(parts)}\n\n{part}")
                if not success:
                    return False
                time.sleep(1)  # Avoid rate limiting
            return True
        else:
            return self._send_single_message(message)
    
    def _send_single_message(self, message: str) -> bool:
        """Send single message to Telegram"""
        url = f"{self.base_url}/sendMessage"
        
        payload = {
            'chat_id': self.chat_id,
            'text': message,
            'parse_mode': 'HTML'
        }
        
        try:
            response = requests.post(url, json=payload, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            if result.get('ok'):
                logger.info("Telegram message sent successfully")
                return True
            else:
                logger.error(f"Telegram API error: {result.get('description')}")
                return False
                
        except Exception as e:
            logger.error(f"Error sending Telegram message: {e}")
            return False

class PumpAnalysisSystem:
    """Main pump analysis system orchestrator"""
    
    def __init__(self):
        # Load environment variables
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.telegram_bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
        self.telegram_chat_id = os.getenv('TELEGRAM_CHAT_ID')
        
        # Validate required environment variables
        if not all([self.openai_api_key, self.telegram_bot_token, self.telegram_chat_id]):
            raise ValueError("Missing required environment variables. Please check OPENAI_API_KEY, TELEGRAM_BOT_TOKEN, and TELEGRAM_CHAT_ID")
        
        # Initialize components
        self.bybit = BybitDataFetcher()
        self.pump_detector = PumpDetector(min_increase_pct=15.0, detection_window_minutes=30)
        self.pre_pump_analyzer = PrePumpAnalyzer(self.bybit)
        self.gpt_analyzer = GPTAnalyzer(self.openai_api_key)
        self.telegram = TelegramNotifier(self.telegram_bot_token, self.telegram_chat_id)
        
        # Initialize learning system
        self.learning_system = LearningSystem()
        logger.info("üß† Learning system initialized")
        
        # Create data directory
        os.makedirs('pump_data', exist_ok=True)
    
    def _get_symbols_from_crypto_scan(self) -> List[str]:
        """Get symbols using unlimited approach - first try API, then use comprehensive fallback"""
        try:
            # First try crypto-scan's cache method
            import sys
            sys.path.append('../crypto-scan')
            from utils.data_fetchers import get_symbols_cached
            
            symbols = get_symbols_cached(require_chain=False)
            if symbols and len(symbols) > 50:  # Valid cache with many symbols
                logger.info(f"üéØ Retrieved {len(symbols)} symbols from crypto-scan cache")
                return symbols
                
        except Exception as e:
            logger.debug(f"Crypto-scan method failed: {e}")
        
        # Try our Bybit API fetcher
        try:
            symbols = self.bybit.get_active_symbols()
            if symbols and len(symbols) > 50:  # Valid API response
                logger.info(f"üéØ Retrieved {len(symbols)} symbols from Bybit API")
                return symbols
        except Exception as e:
            logger.debug(f"Bybit API failed: {e}")
        
        # Use comprehensive production-ready fallback list (200+ symbols)
        comprehensive_symbols = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT', 'XRPUSDT', 'DOTUSDT', 'LINKUSDT', 'LTCUSDT', 'BCHUSDT',
            'XLMUSDT', 'UNIUSDT', 'FILUSDT', 'TRXUSDT', 'ETCUSDT', 'NEARUSDT', 'ATOMUSDT', 'ALGOUSDT', 'VETUSDT', 'ICPUSDT',
            'THETAUSDT', 'HBARUSDT', 'EGLDUSDT', 'AAVEUSDT', 'EOSUSDT', 'AXSUSDT', 'SANDUSDT', 'MANAUSDT', 'GALAUSDT', 'PAWSUSDT',
            'MATICUSDT', 'AVAXUSDT', 'FTMUSDT', 'LUNAUSDT', 'ONEUSDT', 'ZILUSDT', 'WAVESUSDT', 'ENJUSDT', 'CHZUSDT', 'BATUSDT',
            'ZECUSDT', 'DASHUSDT', 'COMPUSDT', 'YFIUSDT', 'SNXUSDT', 'MKRUSDT', 'RUNEUSDT', 'SUSHIUSDT', 'CRVUSDT', 'BANDUSDT',
            'STORJUSDT', 'BALUSDT', 'KNCUSDT', 'LRCUSDT', 'RLCUSDT', 'RENUSDT', 'NKNUSDT', 'KAVAUSDT', 'IOTAUSDT', 'ONTUSDT',
            'QTUMUSDT', 'ZILUSDT', 'LSKUSDT', 'NANOUSDT', 'SCUSDT', 'DGBUSDT', 'BTTUSDT', 'HOTUSDT', 'WINUSDT', 'CELRUSDT',
            'TOMOUSDT', 'FETUSDT', 'TFUELUSDT', 'DUSKUSDT', 'NUUSDT', 'COCOSUSDT', 'CTXCUSDT', 'HIVEUSDT', 'STPTUSDT', 'ARDRUSDT',
            'DREPUSDT', 'LTOLUSDT', 'ERNUSDT', 'KMDUSDT', 'SCRTUSDT', 'CTSIUSDT', 'MBLRUSDT', 'DOCKUSDT', 'POLYUSDT', 'DATASETUSDT',
            'COTIUSDT', 'KEYUSDT', 'VITEUSDT', 'FTTUSDT', 'OXTUSDT', 'LSKUSDT', 'BQXUSDT', 'REQUSDT', 'BCDUSDT', 'LENDUSDT',
            'OCEANUSDT', 'BEAMUSDT', 'CAKEUSDT', 'SPARTAUSDT', 'TLMUSDT', 'INJUSDT', '1INCHUSDT', 'REEFUSDT', 'OGUSDT', 'ATMUSDT',
            'ASRUSDT', 'CETUSDT', 'RIFUSDT', 'CKBUSDT', 'FIRUSDT', 'LITUSDT', 'SFP', 'DYDXUSDT', 'GALAUSDT', 'LPTUSDT',
            'PONDUSDT', 'DEGOUSDT', 'ALICEUSDT', 'LINEARUSDT', 'MASKUSDT', 'CLVUSDT', 'LRCUSDT', 'XVGUSDT', 'ATARUSDT', 'GTCUSDT',
            'TORNUSDT', 'KEEPUSDT', 'ERNUSDT', 'KLAYUSDT', 'PHAUSDT', 'BONDUSDT', 'MLNUSDT', 'DEXEUSDT', 'C98USDT', 'IOTXUSDT',
            'RAYUSDT', 'LPLUSDT', 'FARMAUSDT', 'ALPACAUSDT', 'QUICKUSDT', 'MBOXUSDT', 'FORUSDT', 'REQUSDT', 'GHSTUSDT', 'WAXPUSDT',
            'TRIBEUSDT', 'GNOUSDT', 'XECUSDT', 'ELFUSDT', 'DYDXUSDT', 'POLYUSDT', 'IDEXUSDT', 'VIDTUSDT', 'UPIUSDT', 'STMXUSDT',
            'RIFUSDT', 'XTZUSDT', 'CHRUSDT', 'MANAUSDT', 'INJUSDT', 'DIAUSDT', 'ANCUSDT', 'BNXUSDT', 'RGTUSDT', 'MOVRUSDT',
            'CITYUSDT', 'ENSUSDT', 'KP3RUSDT', 'QIUSDT', 'PORTOUSDT', 'POWRUSDT', 'VGXUSDT', 'JASMYUSDT', 'AMPUSDT', 'PLAUSDT',
            'PYRUSDT', 'RAREUSDT', 'LAZIOUSDT', 'ACHUSDT', 'IMXUSDT', 'GLMRUSDT', 'LOKAUSDT', 'SCRTUSDT', 'API3USDT', 'BTTCUSDT',
            'ACMUSDT', 'BADGERUSDT', 'FISUSDT', 'OMUSDT', 'PONDUSDT', 'DEGOUSDT', 'ALICEUSDT', 'CHZUSDT', 'SANDUSDT', 'AXSUSDT'
        ]
        
        logger.info(f"üéØ Using comprehensive fallback: {len(comprehensive_symbols)} symbols (production-ready)")
        return comprehensive_symbols
        
    def run_analysis(self, days_back: float = 7, max_symbols: int = 999999):
        """
        Run complete pump analysis
        
        Args:
            days_back: Number of days to analyze
            max_symbols: Maximum number of symbols to analyze
        """
        
        logger.info("üöÄ Starting Pump Analysis System")
        logger.info(f"üìä Analyzing {days_back} days of data for up to {max_symbols} symbols")
        
        # Get active symbols using proven crypto-scan method (unlimited)
        symbols = self._get_symbols_from_crypto_scan()
        
        if not symbols:
            logger.error("‚ùå No symbols retrieved from Bybit")
            return
            
        # Limit symbols if max_symbols is specified
        if max_symbols and max_symbols < len(symbols):
            symbols = symbols[:max_symbols]
            logger.info(f"üìà Limited to {len(symbols)} symbols for analysis: {symbols[:10]}...")
        else:
            logger.info(f"üìà Processing all {len(symbols)} symbols: {symbols[:10]}...")
        
        total_pumps_found = 0
        total_analyses_sent = 0
        
        for i, symbol in enumerate(symbols):
            logger.info(f"üîç Analyzing {symbol} ({i+1}/{len(symbols)})")
            
            try:
                # Get historical data
                end_time = int(datetime.now().timestamp() * 1000)
                start_time = end_time - (days_back * 24 * 60 * 60 * 1000)  # days_back days ago
                
                kline_data = self.bybit.get_kline_data(
                    symbol=symbol,
                    interval="5",
                    start_time=start_time,
                    limit=1000
                )
                
                if not kline_data:
                    logger.warning(f"‚ö†Ô∏è No data for {symbol}")
                    continue
                
                # Detect pumps
                pumps = self.pump_detector.detect_pumps_in_data(kline_data, symbol)
                
                if pumps:
                    logger.info(f"üéØ Found {len(pumps)} pump(s) in {symbol}")
                    total_pumps_found += len(pumps)
                    
                    # Analyze each pump
                    for pump_idx, pump in enumerate(pumps):
                        logger.info(f"üîç Processing pump {pump_idx+1}/{len(pumps)} for {symbol}: +{pump.price_increase_pct:.1f}% at {pump.start_time}")
                        
                        try:
                            # Get pre-pump analysis
                            logger.info(f"üìä Analyzing pre-pump conditions for {symbol} pump...")
                            pre_pump_analysis = self.pre_pump_analyzer.analyze_pre_pump_conditions(pump)
                            
                            if pre_pump_analysis:
                                logger.info(f"‚úÖ Pre-pump analysis successful for {symbol}")
                                
                                # üß† LEARNING SYSTEM INTEGRATION - Test existing functions on new pump
                                logger.info(f"üß™ Testing existing functions on new pump {symbol}...")
                                learning_test_results = {}
                                try:
                                    # Get pre-pump candle data for testing
                                    pre_pump_candles = self._get_pre_pump_candles_for_testing(pump)
                                    if pre_pump_candles is not None:
                                        learning_test_results = self.learning_system.test_functions_on_new_pump(
                                            {
                                                'symbol': pump.symbol,
                                                'start_time': pump.start_time.isoformat(),
                                                'price_increase_pct': pump.price_increase_pct,
                                                'duration_minutes': pump.duration_minutes
                                            },
                                            pre_pump_candles
                                        )
                                        logger.info(f"üìä Learning test complete: {learning_test_results['functions_tested']} functions tested, {len(learning_test_results['successful_detections'])} detected")
                                except Exception as e:
                                    logger.warning(f"‚ö†Ô∏è Learning system test failed: {e}")
                                
                                # Generate GPT analysis
                                logger.info(f"ü§ñ Generating GPT analysis for {symbol}...")
                                gpt_analysis = self.gpt_analyzer.generate_pump_analysis(pre_pump_analysis)
                                
                                # Generate Python detector function
                                logger.info(f"üêç Generating Python detector function for {symbol}...")
                                detector_function = self.gpt_analyzer.generate_detector_function(pre_pump_analysis, pump)
                                
                                # üß† LEARNING SYSTEM INTEGRATION - Save GPT function with metadata
                                logger.info(f"üíæ Saving function to learning system...")
                                active_signals = self._extract_active_signals(pre_pump_analysis)
                                try:
                                    function_path = self.learning_system.save_gpt_function(
                                        detector_function,
                                        pump.symbol,
                                        pump.start_time.strftime('%Y%m%d'),
                                        active_signals,
                                        {
                                            'pump_event': {
                                                'price_increase_pct': pump.price_increase_pct,
                                                'duration_minutes': pump.duration_minutes
                                            },
                                            'analysis': pre_pump_analysis
                                        }
                                    )
                                    logger.info(f"‚úÖ Function saved to learning system: {function_path}")
                                except Exception as e:
                                    logger.error(f"‚ùå Failed to save function to learning system: {e}")
                                
                                # Test the generated detector function
                                logger.info(f"üß™ Testing generated detector function for {symbol}...")
                                test_result = self._test_detector_function(pump, pre_pump_analysis, detector_function)
                                
                                # Format message for Telegram (including test result and learning results)
                                telegram_message = self._format_telegram_message(pump, gpt_analysis, test_result if test_result else {}, learning_test_results)
                                
                                # Send to Telegram
                                logger.info(f"üì§ Sending Telegram message for {symbol}...")
                                if self.telegram.send_message(telegram_message):
                                    total_analyses_sent += 1
                                    logger.info(f"‚úÖ Analysis sent for {symbol} pump at {pump.start_time}")
                                    
                                    # Save analysis to file
                                    self._save_analysis_to_file(pump, pre_pump_analysis, gpt_analysis)
                                else:
                                    logger.error(f"‚ùå Failed to send analysis for {symbol} - Telegram error")
                                
                                # Small delay to avoid overwhelming Telegram
                                time.sleep(2)
                            else:
                                logger.warning(f"‚ö†Ô∏è PUMP SKIPPED: Pre-pump analysis returned None for {symbol} pump +{pump.price_increase_pct:.1f}%")
                                logger.warning(f"   Pump details: start={pump.start_time}, price_before={pump.price_before:.6f}, price_peak={pump.price_peak:.6f}")
                                
                        except Exception as e:
                            logger.error(f"‚ùå Error analyzing pump for {symbol}: {e}")
                            logger.error(f"   Pump details: +{pump.price_increase_pct:.1f}% at {pump.start_time}")
                            continue
                
                # Small delay between symbols
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"‚ùå Error processing {symbol}: {e}")
                continue
        
        logger.info(f"üèÅ Analysis complete!")
        logger.info(f"üìä Total pumps found: {total_pumps_found}")
        logger.info(f"üì§ Total analyses sent: {total_analyses_sent}")
        
        # Send summary to Telegram
        summary_message = f"""
üèÅ <b>PUMP ANALYSIS - PODSUMOWANIE</b>

üìä Przeanalizowane symbole: {len(symbols)}
üéØ Znalezione pumpy: {total_pumps_found}
üì§ Wys≈Çane analizy: {total_analyses_sent}
‚è∞ Czas zako≈Ñczenia: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

‚úÖ Analiza zako≈Ñczona pomy≈õlnie!
        """
        
        self.telegram.send_message(summary_message)
    
    def _format_telegram_message(self, pump: PumpEvent, gpt_analysis: str, test_result: dict = None, learning_results: dict = None) -> str:
        """Format message for Telegram"""
        
        message = f"""
üéØ <b>WYKRYTY PUMP - ANALIZA PRE-PUMP</b>

üí∞ <b>Symbol:</b> {pump.symbol}
üìà <b>Wzrost:</b> +{pump.price_increase_pct:.1f}%
‚è∞ <b>Czas pumpu:</b> {pump.start_time.strftime('%Y-%m-%d %H:%M:%S')} UTC
üíµ <b>Cena przed:</b> ${pump.price_before:.6f}
üöÄ <b>Cena szczyt:</b> ${pump.price_peak:.6f}
üìä <b>Volume spike:</b> {pump.volume_spike:.1f}x

<b>üìù ANALIZA GPT (60 min przed pumpem):</b>

{gpt_analysis}

<b>üß™ TEST WYGENEROWANEJ FUNKCJI:</b>
{self._format_test_result(test_result) if test_result else "‚ùå Test nie zosta≈Ç wykonany"}

---
ü§ñ <i>Automatyczna analiza pump_analysis system</i>
        """
        
        return message
    
    def _save_detector_function(self, pump: PumpEvent, detector_function: str):
        """Save generated detector function to Python file"""
        
        # Extract date from pump event  
        pump_date = pump.start_time.strftime("%Y%m%d")
        symbol = pump.symbol
        
        # Create filename: SYMBOL_YYYYMMDD.py
        filename = f"generated_detectors/{symbol}_{pump_date}.py"
        
        # Clean up function code (remove markdown if present)
        function_code = detector_function
        if "```python" in function_code:
            # Extract code between ```python and ```
            start = function_code.find("```python") + 9
            end = function_code.find("```", start)
            if end != -1:
                function_code = function_code[start:end].strip()
        elif "```" in function_code:
            # Extract code between ``` and ```
            start = function_code.find("```") + 3
            end = function_code.find("```", start)
            if end != -1:
                function_code = function_code[start:end].strip()
        
        # Add header comment
        header = f'''"""
Generated Detector Function - {symbol} {pump_date}
Auto-generated by Pump Analysis System based on real pump data

Pump details:
- Symbol: {symbol}
- Date: {pump.start_time.strftime("%Y-%m-%d %H:%M:%S")}
- Price increase: +{pump.price_increase_pct:.1f}%
- Duration: {pump.duration_minutes} minutes

This function detects pre-pump conditions that preceded the actual pump event.
"""

import pandas as pd
import numpy as np

'''
        
        full_content = header + function_code
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(full_content)
            logger.info(f"üêç Detector function saved to {filename}")
        except Exception as e:
            logger.error(f"‚ùå Error saving detector function: {e}")

    def _save_analysis_to_file(self, pump: PumpEvent, pre_pump_data: Dict, gpt_analysis: str):
        """Save analysis to JSON file"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"pump_data/{pump.symbol}_{timestamp}_pump_analysis.json"
        
        analysis_data = {
            'pump_event': {
                'symbol': pump.symbol,
                'start_time': pump.start_time.isoformat(),
                'price_before': pump.price_before,
                'price_peak': pump.price_peak,
                'price_increase_pct': pump.price_increase_pct,
                'duration_minutes': pump.duration_minutes,
                'volume_spike': pump.volume_spike
            },
            'pre_pump_analysis': pre_pump_data,
            'gpt_analysis': gpt_analysis,
            'analysis_timestamp': datetime.now().isoformat()
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"üíæ Analysis saved to {filename}")
        except Exception as e:
            logger.error(f"‚ùå Error saving analysis to file: {e}")
    
    def _test_detector_function(self, pump: PumpEvent, pre_pump_data: Dict, detector_function: str) -> dict:
        """Test the generated detector function on pre-pump data"""
        
        try:
            # Extract function name from the detector code
            import re
            function_match = re.search(r'def\s+(\w+)\s*\(', detector_function)
            if not function_match:
                return {
                    'success': False,
                    'error': 'Nie znaleziono nazwy funkcji w kodzie',
                    'detected': False
                }
            
            function_name = function_match.group(1)
            
            # Create a safe execution environment
            namespace = {
                'pd': pd,
                'np': np,
                '__builtins__': {},
                'len': len,
                'min': min,
                'max': max,
                'abs': abs,
                'sum': sum,
                'any': any,
                'all': all
            }
            
            # Execute the detector function
            exec(detector_function, namespace)
            
            # Create test DataFrame from pre-pump data
            if 'kline_data' in pre_pump_data:
                test_df = pd.DataFrame(pre_pump_data['kline_data'])
            else:
                # Create minimal test data based on available metrics
                test_data = []
                for i in range(12):  # 12 x 5min = 60 minutes
                    test_data.append({
                        'open': 1.0,
                        'high': 1.02,
                        'low': 0.98,
                        'close': 1.01,
                        'volume': 1000,
                        'timestamp': int(time.time()) - (12-i)*300
                    })
                test_df = pd.DataFrame(test_data)
            
            # Test the function
            detector_func = namespace[function_name]
            result = detector_func(test_df)
            
            # Determine if detection was successful
            detected = bool(result) if isinstance(result, (bool, int, float)) else bool(result.get('detected', False)) if isinstance(result, dict) else False
            
            return {
                'success': True,
                'detected': detected,
                'function_name': function_name,
                'result': result
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'detected': False
            }
    
    def _format_test_result(self, test_result: dict) -> str:
        """Format test result for display"""
        
        if not test_result or not test_result.get('success', False):
            error = test_result.get('error', 'Nieznany b≈ÇƒÖd') if test_result else 'Brak wyniku testu'
            return f"‚ùå <b>B≈ÅƒÑD TESTU:</b> {error}"
        
        detected = test_result.get('detected', False)
        function_name = test_result.get('function_name', 'unknown')
        
        if detected:
            return f"‚úÖ <b>SUKCES:</b> Funkcja <code>{function_name}()</code> wykry≈Ça pre-pump warunki!"
        else:
            return f"‚ö†Ô∏è <b>UWAGA:</b> Funkcja <code>{function_name}()</code> NIE wykry≈Ça pre-pump warunk√≥w"

def main():
    """Main function to run pump analysis"""
    
    # Load environment variables from .env file
    from dotenv import load_dotenv
    load_dotenv()
    
    try:
        # Initialize and run analysis system
        system = PumpAnalysisSystem()
        
        # Run analysis for last 7 days, unlimited symbols
        system.run_analysis(days_back=7)
        
    except Exception as e:
        logger.error(f"‚ùå System error: {e}")
        raise

if __name__ == "__main__":
    main()