WdraÅ¼amy punkt 6 z planu integracji Vision-AI z trend-mode:
ğŸ§  Feedback loop dla modelu CLIP, ktÃ³ry automatycznie analizuje skutecznoÅ›Ä‡ predykcji na podstawie rzeczywistych wynikÃ³w rynku i dostraja model pod kÄ…tem rozpoznawania wÅ‚aÅ›ciwych faz rynku.

âœ… Cel:
Analiza trafnoÅ›ci predykcji fazy rynku (np. breakout, pullback, accumulation) wygenerowanej przez CLIP.

JeÅ›li predykcja byÅ‚a nietrafna, system oznacza jÄ… jako bÅ‚Ä…d i uÅ¼ywa tego jako prÃ³bki treningowej do fine-tune modelu CLIP.

ğŸ“‚ Plik: clip_feedback_loop.py (NOWY)
python
Kopiuj
Edytuj
import os
import json
import torch
from clip_model import load_clip_model, preprocess_image
from clip_trainer import train_clip_on_sample

HISTORY_PATH = "logs/auto_label_session_history.json"
MODEL_PATH = "models/clip_model.pt"

def load_feedback_history():
    if not os.path.exists(HISTORY_PATH):
        return []
    with open(HISTORY_PATH, "r") as f:
        return json.load(f)

def analyze_prediction_success(training_data, tjde_results):
    # PorÃ³wnaj predykcjÄ™ CLIP z faktycznÄ… etykietÄ… (np. z GPT lub wyniku rynku)
    incorrect = []
    for item in training_data:
        symbol = item["symbol"]
        label = item["label"]  # np. "pullback-in-trend"
        actual_result = tjde_results.get(symbol)

        if actual_result:
            actual_decision = actual_result.get("decision")
            if label == "breakout-continuation" and actual_decision != "consider_entry":
                incorrect.append(item)
            elif label == "trend-reversal" and actual_decision != "avoid":
                incorrect.append(item)
            # Dodaj inne reguÅ‚y interpretacji w zaleÅ¼noÅ›ci od strategii

    return incorrect

def feedback_loop_clip():
    print("[CLIP FEEDBACK] Starting feedback analysis...")
    model, preprocess = load_clip_model()

    feedback_data = load_feedback_history()
    tjde_results_path = sorted([
        f for f in os.listdir("data/results") if f.startswith("tjde_results_")
    ])[-1]  # Najnowszy wynik TJDE

    with open(f"data/results/{tjde_results_path}", "r") as f:
        tjde_results = {entry["symbol"]: entry for entry in json.load(f)}

    incorrect_samples = analyze_prediction_success(feedback_data, tjde_results)

    if not incorrect_samples:
        print("[CLIP FEEDBACK] No incorrect predictions found.")
        return

    print(f"[CLIP FEEDBACK] Found {len(incorrect_samples)} incorrect predictions. Retraining...")

    for sample in incorrect_samples:
        img_path = f"data/training/charts/{sample['filename']}.png"
        labels = sample["label"].split(" | ")
        train_clip_on_sample(model, preprocess_image(img_path), labels)

    torch.save(model.state_dict(), MODEL_PATH)
    print("[CLIP FEEDBACK] Model fine-tuned and saved.")

ğŸ” Wymagane powiÄ…zania:
clip_model.py: Å‚adowanie modelu i tokenizerÃ³w CLIP.

clip_trainer.py: musi zawieraÄ‡ funkcjÄ™ train_clip_on_sample(model, image_tensor, labels).

logs/auto_label_session_history.json: zawiera listÄ™ predykcji CLIP wraz z nazwÄ… pliku i etykietÄ… np.:

json
Kopiuj
Edytuj
[
  {
    "symbol": "XAUTUSDT",
    "filename": "XAUTUSDT_20250623_1652",
    "label": "pullback-in-trend | trending-up | volume-backed"
  },
  ...
]
âœ… Checklist:
 Analiza skutecznoÅ›ci CLIP na podstawie najnowszych wynikÃ³w TJDE

 Trening CLIP na bÅ‚Ä™dnych predykcjach

 Zapisywanie nowego modelu

 MoÅ¼liwoÅ›Ä‡ cyklicznego uruchamiania np. raz dziennie

Daj znaÄ‡, jeÅ›li sÄ… potrzebne dodatkowe funkcje typu: logowanie bÅ‚Ä™dnych etykiet do pliku lub wsparcie dla wandb/logginga.

DziÄ™ki!